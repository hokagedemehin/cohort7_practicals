{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "source": [
    "import argparse\r\n",
    "import csv\r\n",
    "import itertools\r\n",
    "import logging\r\n",
    "import time\r\n",
    "\r\n",
    "import requests\r\n",
    "import yaml\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "\r\n",
    "logging.root.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "source": [
    "with open('env.yml', 'r') as stream:\r\n",
    "    ENV = yaml.safe_load(stream)\r\n",
    "print(ENV)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'ARTICLE_COUNT_SPAN': 'lx-pagination__page-number qa-pagination-total-page-number', 'HEADLINE_SPAN_CLASS_A': 'bbc-13dm3d0 e1yj3cbb0', 'HEADLINE_SPAN_CLASS_B': None, 'STORY_DIV_CLASS': 'bbc-19j92fr e57qer20', 'CATEGORY_URLS': {'MOST_POPULAR': 'https://www.bbc.com/pidgin/popular/read', 'NIGERIA': 'https://www.bbc.com/pidgin/topics/c2dwqd1zr92t', 'AFRICA': 'https://www.bbc.com/pidgin/topics/c404v061z85t', 'WORLD': 'https://www.bbc.com/pidgin/topics/c0823e52dd0t', 'SPORT': 'https://www.bbc.com/pidgin/topics/cjgn7gv77vrt', 'ENTERTAINMENT': 'https://www.bbc.com/pidgin/topics/cqywjyzk2vyt'}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "source": [
    "ENV['CATEGORY_URLS']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'MOST_POPULAR': 'https://www.bbc.com/pidgin/popular/read',\n",
       " 'NIGERIA': 'https://www.bbc.com/pidgin/topics/c2dwqd1zr92t',\n",
       " 'AFRICA': 'https://www.bbc.com/pidgin/topics/c404v061z85t',\n",
       " 'WORLD': 'https://www.bbc.com/pidgin/topics/c0823e52dd0t',\n",
       " 'SPORT': 'https://www.bbc.com/pidgin/topics/cjgn7gv77vrt',\n",
       " 'ENTERTAINMENT': 'https://www.bbc.com/pidgin/topics/cqywjyzk2vyt'}"
      ]
     },
     "metadata": {},
     "execution_count": 263
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aa = list(ENV['CATEGORY_URLS'].keys())\r\n",
    "str(aa).upper().split(\",\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "ALL_CATEGORIES = ENV[\"CATEGORY_URLS\"] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [
    "ALL_CATEGORIES.items()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('NIGERIA', 'https://www.bbc.com/pidgin/topics/c2dwqd1zr92t'), ('AFRICA', 'https://www.bbc.com/pidgin/topics/c404v061z85t'), ('WORLD', 'https://www.bbc.com/pidgin/topics/c0823e52dd0t'), ('VIDEO', 'https://www.bbc.com/pidgin/media/video'), ('SPORT', 'https://www.bbc.com/pidgin/topics/cjgn7gv77vrt'), ('ENTERTAINMENT', 'https://www.bbc.com/pidgin/topics/cqywjyzk2vyt')])"
      ]
     },
     "metadata": {},
     "execution_count": 264
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "for category, urls in ALL_CATEGORIES.items():\r\n",
    "  # urls = str(urls.split(\" \"))\r\n",
    "  print(list(urls.split(\" \")))\r\n",
    "  print(urls)\r\n",
    "  # for url in urls:\r\n",
    "  #   print(url)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['https://www.bbc.com/pidgin/popular/read']\n",
      "https://www.bbc.com/pidgin/popular/read\n",
      "['https://www.bbc.com/pidgin/topics/c2dwqd1zr92t']\n",
      "https://www.bbc.com/pidgin/topics/c2dwqd1zr92t\n",
      "['https://www.bbc.com/pidgin/topics/c404v061z85t']\n",
      "https://www.bbc.com/pidgin/topics/c404v061z85t\n",
      "['https://www.bbc.com/pidgin/topics/c0823e52dd0t']\n",
      "https://www.bbc.com/pidgin/topics/c0823e52dd0t\n",
      "['https://www.bbc.com/pidgin/media/video']\n",
      "https://www.bbc.com/pidgin/media/video\n",
      "['https://www.bbc.com/pidgin/topics/cjgn7gv77vrt']\n",
      "https://www.bbc.com/pidgin/topics/cjgn7gv77vrt\n",
      "['https://www.bbc.com/pidgin/topics/cqywjyzk2vyt']\n",
      "https://www.bbc.com/pidgin/topics/cqywjyzk2vyt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "source": [
    "def get_parser() -> argparse.ArgumentParser:\r\n",
    "    \"\"\"\r\n",
    "    parse command line arguments\r\n",
    "\r\n",
    "    returns:\r\n",
    "        parser - ArgumentParser object\r\n",
    "    \"\"\"\r\n",
    "    parser = argparse.ArgumentParser(description=\"BBC Pidgin Scraper\")\r\n",
    "    parser.add_argument(\r\n",
    "        \"--output_file_name\",\r\n",
    "        type=str,\r\n",
    "        default=\"bbc_pidgin_corpus.csv\",\r\n",
    "        help=\"Name of output file\",\r\n",
    "    )\r\n",
    "    parser.add_argument(\r\n",
    "        \"--categories\",\r\n",
    "        type=list,\r\n",
    "        default=list(ENV['CATEGORY_URLS'].keys()),\r\n",
    "        help=\"This is a list of keys of the categories\"\r\n",
    "    )\r\n",
    "    parser.add_argument(\r\n",
    "        \"--time_delay\",\r\n",
    "        # type=bool,\r\n",
    "        # action=\"store_false\",\r\n",
    "        default=True,\r\n",
    "        help=\"Time delay is set to true by default\"\r\n",
    "    )\r\n",
    "    parser.add_argument(\r\n",
    "        \"--no_of_articles\",\r\n",
    "        type=int,\r\n",
    "        default=10,\r\n",
    "        help=\"This is the number of articles\"\r\n",
    "    )\r\n",
    "    # TODO: Add your other command line arguments!\r\n",
    "    #   Be sure to include their types & any default values.\r\n",
    "    return parser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "source": [
    "parser = get_parser()\r\n",
    "params, unknown = parser.parse_known_args()\r\n",
    "print(params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(output_file_name='bbc_pidgin_corpus.csv', categories=['MOST_POPULAR', 'NIGERIA', 'AFRICA', 'WORLD', 'SPORT', 'ENTERTAINMENT'], time_delay=True, no_of_articles=10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "val = \"Demehin, Ibukun\"\r\n",
    "val.upper().split(\",\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['DEMEHIN', ' IBUKUN']"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "categories = params.categories.upper().split(\",\")\r\n",
    "print(categories)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if params.categories != \"all\":\r\n",
    "    categories = params.categories.upper().split(\",\")\r\n",
    "    categories = {category: ALL_CATEGORIES[category] for category in categories}\r\n",
    "else:\r\n",
    "    categories = ALL_CATEGORIES"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "source": [
    "if params.categories:\r\n",
    "    categories = params.categories\r\n",
    "    categories = {category: ALL_CATEGORIES[category] for category in categories}\r\n",
    "else:\r\n",
    "    categories = ALL_CATEGORIES\r\n",
    "print(categories)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'MOST_POPULAR': 'https://www.bbc.com/pidgin/popular/read', 'NIGERIA': 'https://www.bbc.com/pidgin/topics/c2dwqd1zr92t', 'AFRICA': 'https://www.bbc.com/pidgin/topics/c404v061z85t', 'WORLD': 'https://www.bbc.com/pidgin/topics/c0823e52dd0t', 'SPORT': 'https://www.bbc.com/pidgin/topics/cjgn7gv77vrt', 'ENTERTAINMENT': 'https://www.bbc.com/pidgin/topics/cqywjyzk2vyt'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "source": [
    "for category, urls in categories.items():\r\n",
    "  print(urls)\r\n",
    "  # for url in urls:\r\n",
    "  #   print(url)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.bbc.com/pidgin/topics/cjgn7gv77vrt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "source": [
    "def get_page_soup(url: str): # ENV['CATEGORY_URLS']['NIGERIA']\r\n",
    "    \"\"\"\r\n",
    "    Makes a request to a url and creates a beautiful soup object from the response html\r\n",
    "\r\n",
    "    input:\r\n",
    "        :param url: input page url\r\n",
    "    returns:\r\n",
    "        - page_soup: beautiful soup object from the response html\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    response = requests.get(url)\r\n",
    "    page_soup = BeautifulSoup(response.text, \"html.parser\")\r\n",
    "    # TODO: Complete the function to parse and return the page\r\n",
    "    return page_soup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = get_page_soup(categories['SPORT'])\r\n",
    "print(page_soup)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "source": [
    "def get_valid_urls(category_page: BeautifulSoup):\r\n",
    "    \"\"\"\r\n",
    "    Gets all valid urls from a category page\r\n",
    "\r\n",
    "    input:\r\n",
    "        :param: url: category_page\r\n",
    "    returns:\r\n",
    "        - valid_urls: list of all valid article urls on a given category page\r\n",
    "    \"\"\"\r\n",
    "    base_url = \"https://www.bbc.com\"\r\n",
    "\r\n",
    "    all_urls = category_page.findAll(\"a\")\r\n",
    "    valid_article_urls = []\r\n",
    "    for url in all_urls:\r\n",
    "        \r\n",
    "        href = url.get(\"href\")\r\n",
    "        if (href.startswith(\"/pidgin/tori\") or \\\r\n",
    "            href.startswith(\"/pidgin/world\") or \\\r\n",
    "            href.startswith(\"/pidgin/sport\")):\r\n",
    "            complete_href = f\"{base_url}{href}\"\r\n",
    "            valid_article_urls.append(complete_href)\r\n",
    "        \r\n",
    "        # from a look at BBC pidgin's urls, they always begin with the following strings.\r\n",
    "        # so we obtain valid article urls using these strings\r\n",
    "    return list(set(valid_article_urls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "source": [
    "get_valid_urls(page_soup)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.bbc.com/pidgin/tori-58692696',\n",
       " 'https://www.bbc.com/pidgin/sport-58681606',\n",
       " 'https://www.bbc.com/pidgin/sport-58692695',\n",
       " 'https://www.bbc.com/pidgin/sport-58744891',\n",
       " 'https://www.bbc.com/pidgin/sport-58661394',\n",
       " 'https://www.bbc.com/pidgin/sport-58722442',\n",
       " 'https://www.bbc.com/pidgin/sport-58736253',\n",
       " 'https://www.bbc.com/pidgin/sport-58697469',\n",
       " 'https://www.bbc.com/pidgin/sport-58674414',\n",
       " 'https://www.bbc.com/pidgin/sport-58750520']"
      ]
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "source": [
    "def get_urls(category_url: str, category: str, time_delay: bool) -> list: # CATEGORY_URLS, NIGERIA, 3000\r\n",
    "    \"\"\"\r\n",
    "    # TODO: Complete the docstring for this function\r\n",
    "    Get all the urls possible on each page\r\n",
    "    input:\r\n",
    "        :category_url: the list of urlobjects \r\n",
    "        :category: a category that we need to sort the list by \r\n",
    "        :time_delay: A boolean function whether to apply a time delay\r\n",
    "    returns:\r\n",
    "        - category_urls: list of all urls from the CATEGORY_URLS in the env.yml\r\n",
    "    \"\"\"\r\n",
    "    page_soup = get_page_soup(category_url)\r\n",
    "    category_urls = get_valid_urls(page_soup)\r\n",
    "\r\n",
    "    # get total number of pages for given category\r\n",
    "    article_count_span = page_soup.find_all(\r\n",
    "        \"span\", attrs={\"class\": \"lx-pagination__page-number qa-pagination-total-page-number\"}\r\n",
    "    )\r\n",
    "    # if there are multiple pages, get valid urls from each page\r\n",
    "    # else just get the articles on the first page\r\n",
    "    if article_count_span:\r\n",
    "        total_article_count = int(article_count_span[0].text)\r\n",
    "        logging.info(f\"{total_article_count} pages found for {category}\")\r\n",
    "        logging.info(f\"{len(category_urls)} urls in page 1 gotten for {category}\")\r\n",
    "\r\n",
    "        for count in range(1, total_article_count):\r\n",
    "            # TODO: Use your `get_page_soup` and `get_valid_urls` functions\r\n",
    "            #   to obtain valid urls from all pages\r\n",
    "            next_page = f\"{category_url}/page/{count + 1}\"\r\n",
    "            next_page_soup = get_page_soup(next_page)\r\n",
    "            next_category_urls = get_valid_urls(next_page_soup)\r\n",
    "            logging.info(f\"{len(next_category_urls)} urls in page {count + 1} gotten for {category}\")\r\n",
    "            category_urls += next_category_urls\r\n",
    "            if time_delay:\r\n",
    "                time.sleep(10)\r\n",
    "    else:\r\n",
    "        logging.info(f\"Only one page found for {category}. {len(category_urls)} urls gotten\")\r\n",
    "\r\n",
    "    return category_urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "def get_urls(category_url: str, category: str, time_delay: bool) -> list: # CATEGORY_URLS, NIGERIA, 3000\r\n",
    "    \"\"\"\r\n",
    "    # TODO: Complete the docstring for this function\r\n",
    "    Get all the urls possible on each page\r\n",
    "    input:\r\n",
    "        :category_url: the list of urlobjects \r\n",
    "        :category: a category that we need to sort the list by \r\n",
    "        :time_delay: A boolean function whether to apply a time delay\r\n",
    "    returns:\r\n",
    "        - category_urls: list of all urls from the CATEGORY_URLS in the env.yml\r\n",
    "    \"\"\"\r\n",
    "    page_soup = get_page_soup(category_url)\r\n",
    "    category_urls = get_valid_urls(page_soup)\r\n",
    "\r\n",
    "    # print(category_urls)\r\n",
    "    # get total number of pages for given category\r\n",
    "    article_count_span = page_soup.find_all(\r\n",
    "        \"span\", attrs={\"class\": \"lx-pagination__page-number qa-pagination-total-page-number\"}\r\n",
    "    )\r\n",
    "\r\n",
    "    \r\n",
    "    # if there are multiple pages, get valid urls from each page\r\n",
    "    # else just get the articles on the first page\r\n",
    "    if article_count_span:\r\n",
    "        total_article_count = int(article_count_span[0].text)\r\n",
    "        logging.info(f\"{total_article_count} pages found for {category}\")\r\n",
    "        logging.info(f\"{len(category_urls)} urls in page 1 gotten for {category}\")\r\n",
    "    \r\n",
    "    \r\n",
    "\r\n",
    "        for count in range(1, 3):\r\n",
    "            # TODO: Use your `get_page_soup` and `get_valid_urls` functions\r\n",
    "            #   to obtain valid urls from all pages\r\n",
    "            next_page = f\"{category_url}/page/{count + 1}\"\r\n",
    "            next_page_soup = get_page_soup(next_page)\r\n",
    "            next_category_urls = get_valid_urls(next_page_soup)\r\n",
    "            logging.info(f\"{len(next_category_urls)} urls in page {count + 1} gotten for {category}\")\r\n",
    "            category_urls += next_category_urls\r\n",
    "            if time_delay:\r\n",
    "                time.sleep(10)\r\n",
    "    else:\r\n",
    "        logging.info(f\"Only one page found for {category}. {len(category_urls)} urls gotten\")\r\n",
    "\r\n",
    "    return category_urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "source": [
    "get_urls(categories['SPORT'], \"Sport\", False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:45 pages found for Sport\n",
      "INFO:root:10 urls in page 1 gotten for Sport\n",
      "INFO:root:9 urls in page 2 gotten for Sport\n",
      "INFO:root:10 urls in page 3 gotten for Sport\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.bbc.com/pidgin/tori-58692696',\n",
       " 'https://www.bbc.com/pidgin/sport-58681606',\n",
       " 'https://www.bbc.com/pidgin/sport-58692695',\n",
       " 'https://www.bbc.com/pidgin/sport-58744891',\n",
       " 'https://www.bbc.com/pidgin/sport-58661394',\n",
       " 'https://www.bbc.com/pidgin/sport-58722442',\n",
       " 'https://www.bbc.com/pidgin/sport-58736253',\n",
       " 'https://www.bbc.com/pidgin/sport-58697469',\n",
       " 'https://www.bbc.com/pidgin/sport-58674414',\n",
       " 'https://www.bbc.com/pidgin/sport-58750520',\n",
       " 'https://www.bbc.com/pidgin/sport-58600151',\n",
       " 'https://www.bbc.com/pidgin/sport-58466316',\n",
       " 'https://www.bbc.com/pidgin/sport-58426989',\n",
       " 'https://www.bbc.com/pidgin/sport-58406381',\n",
       " 'https://www.bbc.com/pidgin/tori-58479296',\n",
       " 'https://www.bbc.com/pidgin/sport-58612482',\n",
       " 'https://www.bbc.com/pidgin/world-58531069',\n",
       " 'https://www.bbc.com/pidgin/tori-58519837',\n",
       " 'https://www.bbc.com/pidgin/world-58552521',\n",
       " 'https://www.bbc.com/pidgin/tori-58318077',\n",
       " 'https://www.bbc.com/pidgin/sport-58322692',\n",
       " 'https://www.bbc.com/pidgin/sport-58415337',\n",
       " 'https://www.bbc.com/pidgin/sport-58401797',\n",
       " 'https://www.bbc.com/pidgin/sport-58336076',\n",
       " 'https://www.bbc.com/pidgin/tori-58273215',\n",
       " 'https://www.bbc.com/pidgin/sport-58322936',\n",
       " 'https://www.bbc.com/pidgin/sport-58346445',\n",
       " 'https://www.bbc.com/pidgin/sport-58392913',\n",
       " 'https://www.bbc.com/pidgin/sport-58348031']"
      ]
     },
     "metadata": {},
     "execution_count": 231
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "source": [
    "article_urls = get_urls(categories['SPORT'], \"Sport\", False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:45 pages found for Sport\n",
      "INFO:root:10 urls in page 1 gotten for Sport\n",
      "INFO:root:9 urls in page 2 gotten for Sport\n",
      "INFO:root:10 urls in page 3 gotten for Sport\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "def get_article_data(article_url: str) -> tuple:\r\n",
    "    \"\"\"\r\n",
    "    Obtains paragraphs texts and headlines input url article\r\n",
    "\r\n",
    "    input:\r\n",
    "        :param article_url: category_page\r\n",
    "    returns:\r\n",
    "        - headline: headline of url article\r\n",
    "        - story_text: text of url article\r\n",
    "        - article_url: input article url\r\n",
    "    \"\"\"\r\n",
    "    page_soup = get_page_soup(article_url)\r\n",
    "    # TODO: Locate the headline element\r\n",
    "    headline = page_soup.find(\r\n",
    "        \"h1\", attrs={\"class\": ENV[\"HEADLINE_SPAN_CLASS_A\"]}\r\n",
    "    )\r\n",
    "    \r\n",
    "    if headline:\r\n",
    "        headline = headline.text\r\n",
    "        # TODO: Get the headline text from the element\r\n",
    "    # return headline\r\n",
    "\r\n",
    "    story_text = \" \"\r\n",
    "    story_div = page_soup.findAll(\"div\", attrs={\"class\": ENV[\"STORY_DIV_CLASS\"]})    # TODO: Locate all story divs\r\n",
    "    if story_div:\r\n",
    "        all_paragraphs = [div.findAll(\"p\", recursive=False) for div in story_div]\r\n",
    "        all_paragraphs = list(itertools.chain(*all_paragraphs))\r\n",
    "        story_text = story_text.join(str(paragraph) for paragraph in all_paragraphs)\r\n",
    "        story_text = BeautifulSoup(story_text, \"html.parser\").get_text()\r\n",
    "    story_text = story_text if not story_text == \" \" else None\r\n",
    "\r\n",
    "    return headline, story_text, article_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "get_article_data(article_urls[2])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Chelsea vs Man City: Gabriel Jesus goal end Chelsea unbeaten start to Premier League season',\n",
       " 'Gabriel Jesus second half goal help Manchester City defeat Chelsea 1-0 for dia own backyard. With two strikers on di pitch, Chelsea struggle to create chances as City end dia unbeaten start to di 2021/2022 Premier League season. Jesus deflected shot for di 53rd minute break di Chelsea defence wey don look solid dis season. Jesus almost score second goal wen Thiago Silva clear im effort off di line while Aymeric Laporte for score but im somehow slide e shot wide from inside di six-yard box. Mendy also produce two crucial saves from di excellent Jack Grealish. Before di game, Pep Guardiola bin don lose di last three matches wey im don play against Thomas Tuchel Chelsea including last season Champions League final. Now, Guardiola and im boys don get di better of Chelsea as dem dominate di Blues for Stamford Bridge. More dey come.',\n",
       " 'https://www.bbc.com/pidgin/sport-58692695')"
      ]
     },
     "metadata": {},
     "execution_count": 234
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "def scrape(output_file_name: str, no_of_articles: int, category_urls: dict, time_delay: bool) -> None:\r\n",
    "    \"\"\"\r\n",
    "    (params.output_file_name, 100, ALL_CATEGORIES, False)\r\n",
    "\r\n",
    "    Main function for scraping and writing articles to file\r\n",
    "\r\n",
    "    input:\r\n",
    "        :param output_file_name: file name where output is saved\r\n",
    "        :param no_of_articles: number of user specified articles to scrape\r\n",
    "        :param category_urls: all articles in a category\r\n",
    "    \"\"\"\r\n",
    "    logging.info(\"Writing articles to file...\")\r\n",
    "\r\n",
    "    with open(output_file_name, \"w\", encoding='utf-8') as csv_file:\r\n",
    "        fieldnames = ['Headline', 'Paragraph', \"URL\"]\r\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\r\n",
    "\r\n",
    "        writer.writeheader()\r\n",
    "        # TODO: Initialize a csv.DictWriter object\r\n",
    "        #   Write the headers your CSV file will take\r\n",
    "        story_num = 0\r\n",
    "\r\n",
    "        for category, urls in category_urls.items():\r\n",
    "            # print(category)\r\n",
    "            logging.info(f\"Writing articles for {category} category...\")\r\n",
    "            for Url in urls:\r\n",
    "                # TODO: Get the headline, paragraphs and url using your `get_article_data` func\r\n",
    "                headline, paragraphs, url = get_article_data(Url)\r\n",
    "                # print(headline)\r\n",
    "                if paragraphs:\r\n",
    "                    # TODO: Write all the data to CSV\r\n",
    "                    writer.writerow({'Headline': headline, 'Paragraph': paragraphs, 'URL': url})\r\n",
    "                    story_num += 1\r\n",
    "                    logging.info(f\"Successfully wrote story number {story_num}\")\r\n",
    "\r\n",
    "                if story_num == no_of_articles:\r\n",
    "                    logging.info(\r\n",
    "                        f\"Requested total number of articles {no_of_articles} reached\"\r\n",
    "                    )\r\n",
    "                    logging.info(\r\n",
    "                        f\"Scraping done. A total of {no_of_articles} articles were scraped!\"\r\n",
    "                    )\r\n",
    "                    return\r\n",
    "                if time_delay:\r\n",
    "                    time.sleep(10)\r\n",
    "    logging.info(\r\n",
    "        f\"Scraping done. A total of {story_num} articles were scraped!\"\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "category_urls = {}\r\n",
    "for category, url in categories.items():\r\n",
    "    logging.info(f\"Getting all stories for {category}...\")\r\n",
    "    category_story_links = get_urls(url, category, params.time_delay)\r\n",
    "    logging.info(f\"{len(category_story_links)} stories found for {category} category\")\r\n",
    "    category_urls[category] = category_story_links"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Getting all stories for MOST_POPULAR...\n",
      "INFO:root:Only one page found for MOST_POPULAR. 9 urls gotten\n",
      "INFO:root:9 stories found for MOST_POPULAR category\n",
      "INFO:root:Getting all stories for NIGERIA...\n",
      "INFO:root:100 pages found for NIGERIA\n",
      "INFO:root:9 urls in page 1 gotten for NIGERIA\n",
      "INFO:root:10 urls in page 2 gotten for NIGERIA\n",
      "INFO:root:8 urls in page 3 gotten for NIGERIA\n",
      "INFO:root:27 stories found for NIGERIA category\n",
      "INFO:root:Getting all stories for AFRICA...\n",
      "INFO:root:91 pages found for AFRICA\n",
      "INFO:root:8 urls in page 1 gotten for AFRICA\n",
      "INFO:root:10 urls in page 2 gotten for AFRICA\n",
      "INFO:root:10 urls in page 3 gotten for AFRICA\n",
      "INFO:root:28 stories found for AFRICA category\n",
      "INFO:root:Getting all stories for WORLD...\n",
      "INFO:root:100 pages found for WORLD\n",
      "INFO:root:8 urls in page 1 gotten for WORLD\n",
      "INFO:root:10 urls in page 2 gotten for WORLD\n",
      "INFO:root:9 urls in page 3 gotten for WORLD\n",
      "INFO:root:27 stories found for WORLD category\n",
      "INFO:root:Getting all stories for SPORT...\n",
      "INFO:root:45 pages found for SPORT\n",
      "INFO:root:10 urls in page 1 gotten for SPORT\n",
      "INFO:root:9 urls in page 2 gotten for SPORT\n",
      "INFO:root:10 urls in page 3 gotten for SPORT\n",
      "INFO:root:29 stories found for SPORT category\n",
      "INFO:root:Getting all stories for ENTERTAINMENT...\n",
      "INFO:root:84 pages found for ENTERTAINMENT\n",
      "INFO:root:10 urls in page 1 gotten for ENTERTAINMENT\n",
      "INFO:root:7 urls in page 2 gotten for ENTERTAINMENT\n",
      "INFO:root:10 urls in page 3 gotten for ENTERTAINMENT\n",
      "INFO:root:27 stories found for ENTERTAINMENT category\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "scrape(params.output_file_name,100,category_urls,False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Writing articles to file...\n",
      "INFO:root:Writing articles for MOST_POPULAR category...\n",
      "INFO:root:Successfully wrote story number 1\n",
      "INFO:root:Successfully wrote story number 2\n",
      "INFO:root:Successfully wrote story number 3\n",
      "INFO:root:Successfully wrote story number 4\n",
      "INFO:root:Successfully wrote story number 5\n",
      "INFO:root:Successfully wrote story number 6\n",
      "INFO:root:Successfully wrote story number 7\n",
      "INFO:root:Successfully wrote story number 8\n",
      "INFO:root:Successfully wrote story number 9\n",
      "INFO:root:Writing articles for NIGERIA category...\n",
      "INFO:root:Successfully wrote story number 10\n",
      "INFO:root:Successfully wrote story number 11\n",
      "INFO:root:Successfully wrote story number 12\n",
      "INFO:root:Successfully wrote story number 13\n",
      "INFO:root:Successfully wrote story number 14\n",
      "INFO:root:Successfully wrote story number 15\n",
      "INFO:root:Successfully wrote story number 16\n",
      "INFO:root:Successfully wrote story number 17\n",
      "INFO:root:Successfully wrote story number 18\n",
      "INFO:root:Successfully wrote story number 19\n",
      "INFO:root:Successfully wrote story number 20\n",
      "INFO:root:Successfully wrote story number 21\n",
      "INFO:root:Successfully wrote story number 22\n",
      "INFO:root:Successfully wrote story number 23\n",
      "INFO:root:Successfully wrote story number 24\n",
      "INFO:root:Successfully wrote story number 25\n",
      "INFO:root:Successfully wrote story number 26\n",
      "INFO:root:Successfully wrote story number 27\n",
      "INFO:root:Successfully wrote story number 28\n",
      "INFO:root:Successfully wrote story number 29\n",
      "INFO:root:Successfully wrote story number 30\n",
      "INFO:root:Successfully wrote story number 31\n",
      "INFO:root:Successfully wrote story number 32\n",
      "INFO:root:Successfully wrote story number 33\n",
      "INFO:root:Successfully wrote story number 34\n",
      "INFO:root:Successfully wrote story number 35\n",
      "INFO:root:Successfully wrote story number 36\n",
      "INFO:root:Writing articles for AFRICA category...\n",
      "INFO:root:Successfully wrote story number 37\n",
      "INFO:root:Successfully wrote story number 38\n",
      "INFO:root:Successfully wrote story number 39\n",
      "INFO:root:Successfully wrote story number 40\n",
      "INFO:root:Successfully wrote story number 41\n",
      "INFO:root:Successfully wrote story number 42\n",
      "INFO:root:Successfully wrote story number 43\n",
      "INFO:root:Successfully wrote story number 44\n",
      "INFO:root:Successfully wrote story number 45\n",
      "INFO:root:Successfully wrote story number 46\n",
      "INFO:root:Successfully wrote story number 47\n",
      "INFO:root:Successfully wrote story number 48\n",
      "INFO:root:Successfully wrote story number 49\n",
      "INFO:root:Successfully wrote story number 50\n",
      "INFO:root:Successfully wrote story number 51\n",
      "INFO:root:Successfully wrote story number 52\n",
      "INFO:root:Successfully wrote story number 53\n",
      "INFO:root:Successfully wrote story number 54\n",
      "INFO:root:Successfully wrote story number 55\n",
      "INFO:root:Successfully wrote story number 56\n",
      "INFO:root:Successfully wrote story number 57\n",
      "INFO:root:Successfully wrote story number 58\n",
      "INFO:root:Successfully wrote story number 59\n",
      "INFO:root:Successfully wrote story number 60\n",
      "INFO:root:Successfully wrote story number 61\n",
      "INFO:root:Successfully wrote story number 62\n",
      "INFO:root:Successfully wrote story number 63\n",
      "INFO:root:Successfully wrote story number 64\n",
      "INFO:root:Writing articles for WORLD category...\n",
      "INFO:root:Successfully wrote story number 65\n",
      "INFO:root:Successfully wrote story number 66\n",
      "INFO:root:Successfully wrote story number 67\n",
      "INFO:root:Successfully wrote story number 68\n",
      "INFO:root:Successfully wrote story number 69\n",
      "INFO:root:Successfully wrote story number 70\n",
      "INFO:root:Successfully wrote story number 71\n",
      "INFO:root:Successfully wrote story number 72\n",
      "INFO:root:Successfully wrote story number 73\n",
      "INFO:root:Successfully wrote story number 74\n",
      "INFO:root:Successfully wrote story number 75\n",
      "INFO:root:Successfully wrote story number 76\n",
      "INFO:root:Successfully wrote story number 77\n",
      "INFO:root:Successfully wrote story number 78\n",
      "INFO:root:Successfully wrote story number 79\n",
      "INFO:root:Successfully wrote story number 80\n",
      "INFO:root:Successfully wrote story number 81\n",
      "INFO:root:Successfully wrote story number 82\n",
      "INFO:root:Successfully wrote story number 83\n",
      "INFO:root:Successfully wrote story number 84\n",
      "INFO:root:Successfully wrote story number 85\n",
      "INFO:root:Successfully wrote story number 86\n",
      "INFO:root:Successfully wrote story number 87\n",
      "INFO:root:Successfully wrote story number 88\n",
      "INFO:root:Successfully wrote story number 89\n",
      "INFO:root:Successfully wrote story number 90\n",
      "INFO:root:Successfully wrote story number 91\n",
      "INFO:root:Writing articles for SPORT category...\n",
      "INFO:root:Successfully wrote story number 92\n",
      "INFO:root:Successfully wrote story number 93\n",
      "INFO:root:Successfully wrote story number 94\n",
      "INFO:root:Successfully wrote story number 95\n",
      "INFO:root:Successfully wrote story number 96\n",
      "INFO:root:Successfully wrote story number 97\n",
      "INFO:root:Successfully wrote story number 98\n",
      "INFO:root:Successfully wrote story number 99\n",
      "INFO:root:Successfully wrote story number 100\n",
      "INFO:root:Requested total number of articles 100 reached\n",
      "INFO:root:Scraping done. A total of 100 articles were scraped!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('webscraping_and_data_labelling-mY7GX9EY': pipenv)"
  },
  "interpreter": {
   "hash": "c2285d02605903d5f6d072046215f63ff39b6124efff539f45ef2a2024860e3a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}